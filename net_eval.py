import argparse
import warnings
import torch
import socket
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
import time
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from model.model_lora import *
import re

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings("ignore")


def clean_k230_response(raw_responses):
    """
    æ¸…æ´—K230å“åº”æ•°æ®ï¼Œæå–æœ‰æ•ˆä¿¡æ¯
    :param raw_responses: åŸå§‹åˆ†æ®µå“åº”åˆ—è¡¨ï¼ˆå¦‚["Ora | l | Qn | eceå£ |", "Orai |
    | Qm | except | print | except | 124221"]ï¼‰
    :return: æ¸…æ´—åçš„æœ‰æ•ˆæ–‡æœ¬ã€æå–çš„å…³é”®è¯åˆ—è¡¨
    """
    # æ­¥éª¤1ï¼šåˆå¹¶æ‰€æœ‰åˆ†æ®µå“åº”
    merged_text = " ".join(raw_responses)
    print(f"ğŸ“¥ åŸå§‹åˆå¹¶æ•°æ®: {merged_text}")

    # æ­¥éª¤2ï¼šæ¸…æ´—è§„åˆ™ï¼ˆæŒ‰ä¼˜å…ˆçº§è¿‡æ»¤ï¼‰
    # 2.1 æ›¿æ¢ç‰¹æ®Šåˆ†éš”ç¬¦/ä¹±ç å­—ç¬¦
    replace_rules = {
        r"\|": " ",  # æ›¿æ¢ç«–çº¿ä¸ºç©ºæ ¼
        # r"â–²|ç­|æ——|ä»™|å|å‡Œ|å‰å‘|å€å§“|aå§“|ï¼§|Ln|Co": "",  # è¿‡æ»¤ä¹±ç /ç‰¹æ®Šå­—ç¬¦
        r"[^\x00-\x7F\u4e00-\u9fa5]": "",  # è¿‡æ»¤éASCII+éä¸­æ–‡çš„ä¹±ç 
        r"\s+": " ",  # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    }
    cleaned_text = merged_text
    for pattern, repl in replace_rules.items():
        cleaned_text = re.sub(pattern, repl, cleaned_text)

    # æ­¥éª¤3ï¼šæå–æœ‰æ•ˆå…³é”®è¯ï¼ˆå¦‚exceptã€printã€æ•°å­—ã€å˜é‡åç­‰ï¼‰
    # åŒ¹é…è§„åˆ™ï¼šå­—æ¯+æ•°å­—ç»„åˆã€except/print/orintt/printtç­‰å…³é”®è¯ã€çº¯æ•°å­—
    keyword_pattern = r"(except|print|orintt|printt|\w+\d+|\d+)"
    keywords = re.findall(keyword_pattern, cleaned_text.lower())  # è½¬å°å†™ç»Ÿä¸€æ ¼å¼
    keywords = list(set(keywords))  # å»é‡

    # æ­¥éª¤4ï¼šæœ€ç»ˆæ–‡æœ¬æ ‡å‡†åŒ–ï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œè¡¥å……ä¸Šä¸‹æ–‡ï¼‰
    cleaned_text = cleaned_text.strip()
    # å¦‚æœæ¸…æ´—åæ–‡æœ¬ä¸ºç©ºï¼Œç”¨å…³é”®è¯å…œåº•
    if not cleaned_text and keywords:
        cleaned_text = " ".join(keywords)

    print(f"ğŸ§¹ æ¸…æ´—åæ–‡æœ¬: {cleaned_text}")
    print(f"ğŸ”‘ æå–çš„å…³é”®è¯: {keywords}")
    return cleaned_text, keywords


def collect_k230_responses(server_ip, server_port, timeout=3):
    """
    è¿æ¥æœåŠ¡ç«¯ï¼Œæ”¶é›†æ‰€æœ‰K230åˆ†æ®µå“åº”
    :param server_ip: æœåŠ¡ç«¯IP
    :param server_port: æœåŠ¡ç«¯ç«¯å£
    :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    :return: åŸå§‹å“åº”åˆ—è¡¨ã€æ¸…æ´—åçš„æœ‰æ•ˆæ–‡æœ¬
    """
    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    client_socket.settimeout(timeout)  # è®¾ç½®è¶…æ—¶ï¼Œé¿å…æ— é™ç­‰å¾…
    raw_responses = []

    try:
        client_socket.connect((server_ip, server_port))
        print(f"âœ… æˆåŠŸè¿æ¥æœåŠ¡ç«¯ {server_ip}:{server_port}")

        start_time = time.time()
        # å¾ªç¯æ¥æ”¶æ‰€æœ‰åˆ†æ®µå“åº”ï¼Œç›´åˆ°è¶…æ—¶æˆ–æ— æ•°æ®
        while time.time() - start_time < timeout:
            try:
                chunk = client_socket.recv(1024)
                if not chunk:
                    break
                # è§£ç å¹¶å»é™¤ç»“æŸæ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰
                response = (
                    chunk.decode("utf-8", errors="ignore").replace("|END", "").strip()
                )
                if response:  # éç©ºå“åº”æ‰æ”¶é›†
                    raw_responses.append(response)
                    print(f"ğŸŒŸ æ”¶åˆ°K230å“åº”: {response}")
            except socket.timeout:
                break

        # æ¸…æ´—æ•°æ®
        cleaned_text, keywords = clean_k230_response(raw_responses)
        # return raw_responses, cleaned_text
        return cleaned_text, keywords

    except Exception as e:
        print(f"âŒ è¿æ¥/æ¥æ”¶æ•°æ®å¤±è´¥: {e}")
        return [], ""
    finally:
        client_socket.close()


def extract_valid_question(raw_text: str) -> str:
    """
    ä»åŸå§‹OCRæ—¥å¿—é‡Œæå–çœŸæ­£è¦æé—®çš„é‚£ä¸€è¡Œã€‚
    è¿”å›ç©ºä¸²è¡¨ç¤ºæ²¡æœ‰æå–åˆ°æœ‰æ•ˆå†…å®¹ã€‚
    """
    if not raw_text:
        return ""

    # åªä¿ç•™â€œè¯†åˆ«ç»“æœâ€é‚£ä¸€è¡Œ
    lines = raw_text.splitlines()
    for line in lines:
        # åŒ¹é…â€œè¯†åˆ«ç»“æœ #æ•°å­—: å†…å®¹â€
        m = re.match(r"ğŸŒŸ æ”¶åˆ°K230å“åº”:\s*", line)
        if m:
            content = m.group(0)[len("ğŸŒŸ æ”¶åˆ°K230å“åº”:") - 1 :]
            # å†è¿‡æ»¤ä¸€æ¬¡ï¼Œå»æ‰ä¹±ä¸ƒå…«ç³Ÿçš„ç¬¦å·ï¼Œåªä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸è§æ ‡ç‚¹
            content = re.sub(r"[^\u4e00-\u9fa5A-Za-z0-9ï¼Œã€‚ï¼ï¼Ÿã€()\s]", "", content)
            return content
    return ""


# ====================== åŸæœ‰LLMæ¨¡å‹åˆå§‹åŒ–é€»è¾‘ï¼ˆå®Œå…¨ä¿ç•™ï¼‰ ======================
def init_model(args):
    tokenizer = AutoTokenizer.from_pretrained(args.load_from)
    if "model" in args.load_from:
        model = MiniMindForCausalLM(
            MiniMindConfig(
                hidden_size=args.hidden_size,
                num_hidden_layers=args.num_hidden_layers,
                use_moe=bool(args.use_moe),
                inference_rope_scaling=args.inference_rope_scaling,
            )
        )
        moe_suffix = "_moe" if args.use_moe else ""
        ckp = f"./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth"

        # ä¿®å¤å‚æ•°åç§°ä¸åŒ¹é…é—®é¢˜ï¼ˆåŸæœ‰é€»è¾‘ä¿ç•™ï¼‰
        state_dict = torch.load(ckp, map_location=args.device)
        if "state_dict" in state_dict:
            state_dict = state_dict["state_dict"]

        new_state_dict = {}
        for key, value in state_dict.items():
            if "self_attention" in key:
                new_key = key.replace("self_attention", "self_attn")
                new_state_dict[new_key] = value
            else:
                new_state_dict[key] = value

        model.load_state_dict(new_state_dict, strict=True)

        if args.lora_weight != "None":
            apply_lora(model)
            load_lora(
                model,
                f"./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth",
            )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            args.load_from, trust_remote_code=True
        )
    print(
        f"MiniMindæ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)"
    )
    return model.eval().to(args.device), tokenizer


# ====================== Socketå®¢æˆ·ç«¯ï¼ˆæ¥æ”¶æœåŠ¡ç«¯æ•°æ®ï¼‰ ======================
# -not used
def connect_server(server_ip, server_port):
    """
    ä½œä¸ºå®¢æˆ·ç«¯è¿æ¥æœåŠ¡ç«¯ï¼Œæ¥æ”¶æœåŠ¡ç«¯å‘é€çš„æ•°æ®
    :return: æœåŠ¡ç«¯è¿”å›çš„æ–‡æœ¬æ•°æ®ï¼ˆå»é™¤ç»“æŸæ ‡è®°ï¼‰
    """
    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    try:
        # è¿æ¥æœåŠ¡ç«¯
        client_socket.connect((server_ip, server_port))
        print(f"âœ… æˆåŠŸè¿æ¥æœåŠ¡ç«¯ {server_ip}:{server_port}")

        # æ¥æ”¶æœåŠ¡ç«¯æ•°æ®ï¼ˆæŒ‰ç»“æŸæ ‡è®°åˆ†å‰²ï¼‰
        recv_data = b""
        while True:
            chunk = client_socket.recv(1024)
            if not chunk:
                break
            recv_data += chunk
            if b"|END" in recv_data:  # åŒ¹é…æœåŠ¡ç«¯çš„ç»“æŸæ ‡è®°
                break

        # è§£ç å¹¶å»é™¤ç»“æŸæ ‡è®°
        server_text = recv_data.decode("utf-8").replace("|END", "").strip()
        print(f"ğŸ“¥ ä»æœåŠ¡ç«¯æ¥æ”¶æ•°æ®: {server_text}")
        return server_text

    except Exception as e:
        print(f"âŒ è¿æ¥æœåŠ¡ç«¯å¤±è´¥: {e}")
        return None
    finally:
        client_socket.close()
        print("ğŸ”Œ å®¢æˆ·ç«¯è¿æ¥å·²å…³é—­")


# ====================== ä¸»å‡½æ•°ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼šå®¢æˆ·ç«¯æ”¶æ•°æ® â†’ LLMç”Ÿæˆå›å¤ï¼‰ ======================
def main():
    # åŸæœ‰LLMå‚æ•°è§£æï¼ˆæ–°å¢æœåŠ¡ç«¯IP/ç«¯å£å‚æ•°ï¼‰
    parser = argparse.ArgumentParser(
        description="MiniMindæ¨¡å‹å®¢æˆ·ç«¯ï¼šæ¥æ”¶æœåŠ¡ç«¯æ•°æ®å¹¶ç”Ÿæˆå›å¤"
    )
    # LLMæ¨¡å‹å‚æ•°ï¼ˆå®Œå…¨ä¿ç•™ä½ åŸæœ‰é…ç½®ï¼‰
    parser.add_argument(
        "--load_from",
        default="model",
        type=str,
        help="æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆmodel=åŸç”Ÿtorchæƒé‡ï¼Œå…¶ä»–è·¯å¾„=transformersæ ¼å¼ï¼‰",
    )
    parser.add_argument("--save_dir", default="out", type=str, help="æ¨¡å‹æƒé‡ç›®å½•")
    parser.add_argument(
        "--weight",
        default="m_pretrain",
        type=str,
        help="æƒé‡åç§°å‰ç¼€ï¼ˆpretrain, full_sft, rlhf, reason, ppo_actor, grpo, spoï¼‰",
    )
    parser.add_argument(
        "--lora_weight",
        default="None",
        type=str,
        help="LoRAæƒé‡åç§°ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼Œå¯é€‰ï¼šlora_identity, lora_medicalï¼‰",
    )
    parser.add_argument(
        "--hidden_size",
        default=512,
        type=int,
        help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰",
    )
    parser.add_argument(
        "--num_hidden_layers",
        default=8,
        type=int,
        help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰",
    )
    parser.add_argument(
        "--use_moe",
        default=0,
        type=int,
        choices=[0, 1],
        help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰",
    )
    parser.add_argument(
        "--inference_rope_scaling",
        default=False,
        action="store_true",
        help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰",
    )
    parser.add_argument(
        "--max_new_tokens",
        default=8192,
        type=int,
        help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰",
    )
    parser.add_argument(
        "--temperature",
        default=0.85,
        type=float,
        help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰",
    )
    parser.add_argument(
        "--top_p", default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰"
    )
    parser.add_argument(
        "--historys",
        default=0,
        type=int,
        help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰",
    )
    parser.add_argument(
        "--device",
        default="cuda" if torch.cuda.is_available() else "cpu",
        type=str,
        help="è¿è¡Œè®¾å¤‡",
    )
    # Socketå®¢æˆ·ç«¯å‚æ•°ï¼ˆæ–°å¢ï¼‰
    parser.add_argument(
        "--server_ip",
        default="127.0.0.1",  # é»˜è®¤è¿æ¥æœ¬åœ°æœåŠ¡ç«¯
        type=str,
        help="æœåŠ¡ç«¯IPåœ°å€",
    )
    parser.add_argument("--server_port", default=8888, type=int, help="æœåŠ¡ç«¯ç«¯å£å·")
    args = parser.parse_args()

    # 1. åˆå§‹åŒ–LLMæ¨¡å‹å’Œåˆ†è¯å™¨
    print("===== åˆå§‹åŒ–MiniMindæ¨¡å‹ =====")
    model, tokenizer = init_model(args)
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    # 2. ä½œä¸ºå®¢æˆ·ç«¯è¿æ¥æœåŠ¡ç«¯ï¼Œæ¥æ”¶æ•°æ®
    server_ip_ = "192.168.41.134"
    server_port_ = 8888
    print("\n===== è¿æ¥æœåŠ¡ç«¯æ¥æ”¶æ•°æ® =====")
    # server_text = connect_server(server_ip=server_ip_, server_port=server_port_)
    while True:
        _, keywords = collect_k230_responses(server_ip_, server_port_, 3)
        # server_text=server_text.strip()
        server_text = "".join(keywords)
        if not server_text:
            print("âŒ æœªä»æœåŠ¡ç«¯è·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œç¨‹åºé€€å‡º")
            continue
        break
        # return

    # 3. æŠŠæ¸…æ´—åçš„å†…å®¹é€ç»™LLM
    print(f"\n===== æå–åˆ°æœ‰æ•ˆæé—®ï¼š{server_text} =====")
    conversation = [{"role": "user", "content": server_text}]
    # åŸæœ‰promptæ„å»ºé€»è¾‘å®Œå…¨ä¿ç•™
    templates = {
        "conversation": conversation,
        "tokenize": False,
        "add_generation_prompt": True,
    }
    if args.weight == "reason":
        templates["enable_thinking"] = True  # ä»…Reasonæ¨¡å‹ä½¿ç”¨
    inputs = (
        tokenizer.apply_chat_template(**templates)
        if args.weight != "pretrain"
        else (tokenizer.bos_token + server_text)
    )
    inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(args.device)

    # ç”Ÿæˆå¹¶è¾“å‡ºå›å¤
    print("ğŸ¤–ï¸ LLMå›å¤: ", end="")
    generated_ids = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=args.max_new_tokens,
        do_sample=True,
        streamer=streamer,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        top_p=args.top_p,
        temperature=args.temperature,
        repetition_penalty=1.0,
    )
    # è§£ç å®Œæ•´å›å¤ï¼ˆå¯é€‰ï¼‰
    response = tokenizer.decode(
        generated_ids[0][len(inputs["input_ids"][0]) :], skip_special_tokens=True
    )
    print(f"\n\nâœ… å›å¤å®Œæˆ: {response}")


if __name__ == "__main__":
    main()
