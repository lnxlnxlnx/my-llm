import argparse
import warnings
import torch
import socket
import time
import re
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from model.model_lora import *

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings("ignore")


def clean_k230_response(raw_responses):
    """
    æ¸…æ´—K230å“åº”æ•°æ®ï¼šä¿ç•™ä¸­æ–‡+å…³é”®è‹±æ–‡+æ•°å­—ï¼Œè¿‡æ»¤ä¹±ç ç¬¦å·
    """
    # åˆå¹¶åˆ†æ®µå“åº”
    merged_text = " ".join(raw_responses)
    print(f"\nğŸ“¥ K230åŸå§‹åˆå¹¶æ•°æ®: {merged_text}")

    # æ ¸å¿ƒæ¸…æ´—è§„åˆ™ï¼šä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸è§æ ‡ç‚¹ï¼Œè¿‡æ»¤ä¹±ç /ç‰¹æ®Šç¬¦å·
    # ä¿ç•™èŒƒå›´ï¼šä¸­æ–‡(\u4e00-\u9fa5)ã€è‹±æ–‡(a-zA-Z)ã€æ•°å­—(0-9)ã€å…³é”®ç¬¦å·(|:.,()_)ã€å¸¸è§æ ‡ç‚¹
    cleaned_text = re.sub(
        r'[^\u4e00-\u9fa5a-zA-Z0-9|:.,()_\sï¼Œã€‚ï¼ï¼Ÿ]',
        '',
        merged_text
    )
    # æ›¿æ¢ç«–çº¿ä¸ºç©ºæ ¼ï¼Œåˆå¹¶å¤šä½™ç©ºæ ¼
    cleaned_text = cleaned_text.replace("|", " ").strip()
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)

    # æå–å…³é”®å…³é”®è¯ï¼ˆå…œåº•ï¼‰
    keyword_pattern = r"(except|socket|print|connect|recv|send|client|ä½ æ˜¯è°|\d+)"
    keywords = re.findall(keyword_pattern, cleaned_text.lower())
    keywords = list(set(keywords)) if keywords else []

    # ç¡®ä¿æ¸…æ´—åæœ‰æœ‰æ•ˆå†…å®¹
    if not cleaned_text and keywords:
        cleaned_text = " ".join(keywords)
    if not cleaned_text:
        cleaned_text = "æœªè¯†åˆ«åˆ°æœ‰æ•ˆä¿¡æ¯ï¼Œä»…æ£€æµ‹åˆ°ä¹±ç "

    print(f"ğŸ§¹ æ¸…æ´—åæœ‰æ•ˆæ–‡æœ¬: {cleaned_text}")
    return cleaned_text


def collect_and_clean_k230_data(server_ip, server_port, timeout=5):
    """
    å®Œæ•´äº¤äº’ï¼šè¿æ¥K230+å‘é€æ•°æ®+æ¥æ”¶å“åº”+æ¸…æ´—æ•°æ®
    """
    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    client_socket.settimeout(timeout)
    raw_responses = []

    try:
        # 1. è¿æ¥K230æœåŠ¡ç«¯
        print(f"ğŸ”Œ å°è¯•è¿æ¥K230 [{server_ip}:{server_port}]...")
        client_socket.connect((server_ip, server_port))
        print("âœ… æˆåŠŸè¿æ¥K230æœåŠ¡ç«¯ï¼")

        # 2. å‘é€æ•°æ®ï¼ˆåŒ¹é…ä½ æµ‹è¯•ä»£ç çš„å‘é€é€»è¾‘ï¼‰
        send_data = f"PCå®¢æˆ·ç«¯æ¶ˆæ¯: å½“å‰æ—¶é—´ {time.time():.0f}".encode()
        client_socket.send(send_data + b"\n")
        print(f"ğŸ“¤ å·²å‘é€æ•°æ®: {send_data.decode()}")

        # 3. æ¥æ”¶K230å“åº”ï¼ˆåˆ†æ®µæ¥æ”¶ï¼‰
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                chunk = client_socket.recv(1024)
                if not chunk:
                    break
                response = chunk.decode("utf-8", errors="ignore").strip()
                if response:
                    if response == "clash":
                        print("ğŸ˜­æœåŠ¡ç«¯ä¸å‡†å¤‡å‘é€æ•°æ®ï¼")
                        time.sleep(3)
                        return "", ""
                    raw_responses.append(response)
                    print(f"ğŸŒŸ æ”¶åˆ°K230å“åº”: {response}")
            except socket.timeout:
                break

        # 4. æ¸…æ´—æ•°æ®
        cleaned_text = clean_k230_response(raw_responses)
        return cleaned_text

    except socket.timeout:
        print("âŒ è¿æ¥/æ¥æ”¶è¶…æ—¶ï¼è¯·æ£€æŸ¥K230æœåŠ¡ç«¯çŠ¶æ€")
        return ""
    except ConnectionRefusedError:
        print("âŒ è¿æ¥è¢«æ‹’ç»ï¼è¯·ç¡®è®¤K230æœåŠ¡ç«¯å·²å¯åŠ¨")
        return ""
    except Exception as e:
        print(f"âŒ K230äº¤äº’å¼‚å¸¸: {e}")
        return ""
    finally:
        client_socket.close()
        print("ğŸ”Œ å·²å…³é—­K230è¿æ¥")


def init_model(args):
    """
    åˆå§‹åŒ–LLMæ¨¡å‹ï¼šä¿®å¤å‚æ•°åç§°+é€‚é…ä¸­æ–‡
    """
    tokenizer = AutoTokenizer.from_pretrained(args.load_from)
    # é€‚é…ä¸­æ–‡ï¼šè¡¥å……ç¼ºå¤±çš„token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    if "model" in args.load_from:
        model = MiniMindForCausalLM(
            MiniMindConfig(
                hidden_size=args.hidden_size,
                num_hidden_layers=args.num_hidden_layers,
                use_moe=bool(args.use_moe),
                inference_rope_scaling=args.inference_rope_scaling,
            )
        )
        # åŠ è½½æƒé‡å¹¶ä¿®å¤å‚æ•°åç§°
        moe_suffix = "_moe" if args.use_moe else ""
        ckp = f"./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth"
        new_state_dict = torch.load(ckp, map_location=args.device)
        model.load_state_dict(new_state_dict, strict=True)

        # åŠ è½½LoRAï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.lora_weight != "None":
            apply_lora(model)
            load_lora(model, f"./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth")
    else:
        model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)

    # æ‰“å°æ¨¡å‹å‚æ•°ä¿¡æ¯
    param_num = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"\nğŸ”§ MiniMindæ¨¡å‹åŠ è½½å®Œæˆ | å‚æ•°æ€»é‡: {param_num:.2f} M")
    return model.eval().to(args.device), tokenizer


def main():
    # å‚æ•°è§£æï¼ˆç²¾ç®€æ— ç”¨å‚æ•°ï¼‰
    parser = argparse.ArgumentParser(description="MiniMind + K230 äº¤äº’æ¨ç†")
    # LLMæ ¸å¿ƒå‚æ•°
    parser.add_argument("--load_from", default="model", type=str, help="æ¨¡å‹åŠ è½½è·¯å¾„")
    parser.add_argument("--save_dir", default="out", type=str, help="æƒé‡ç›®å½•")
    parser.add_argument("--weight", default="pretrain", type=str, help="æƒé‡å‰ç¼€")
    parser.add_argument("--lora_weight", default="None", type=str, help="LoRAæƒé‡")
    parser.add_argument("--hidden_size", default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument("--num_hidden_layers", default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument("--use_moe", default=0, type=int, choices=[0,1], help="æ˜¯å¦MoEæ¶æ„")
    parser.add_argument("--inference_rope_scaling", default=False, action="store_true", help="RoPEå¤–æ¨")
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--max_new_tokens", default=512, type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--temperature", default=0.7, type=float, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--top_p", default=0.8, type=float, help="Top-Pé‡‡æ ·")
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu", type=str)
    # K230è¿æ¥å‚æ•°
    parser.add_argument("--k230_ip", default="192.168.41.134", type=str, help="K230 IP")
    parser.add_argument("--k230_port", default=8888, type=int, help="K230 ç«¯å£")
    args = parser.parse_args()

    # 1. åˆå§‹åŒ–LLMæ¨¡å‹
    model, tokenizer = init_model(args)
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    while True:
        # client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # client_socket.settimeout(3)
        # print(f"ğŸ”Œ å°è¯•è¿æ¥K230 [{args.server_ip}:{args.server_port}]...")
        # client_socket.connect((args.server_ip, args.server_port))
        # print("âœ… æˆåŠŸè¿æ¥K230æœåŠ¡ç«¯ï¼")
        # 2. ä¸K230äº¤äº’ï¼Œè·å–å¹¶æ¸…æ´—æ•°æ®
        k230_text = collect_and_clean_k230_data(args.k230_ip, args.k230_port)
        if not k230_text:
            print("\nâŒ æ— æœ‰æ•ˆK230æ•°æ®ï¼Œé€€å‡ºæ¨ç†")
            return

        # 3. æ„å»ºLLMè¾“å…¥Promptï¼ˆé€‚é…äº¤æµåœºæ™¯ï¼‰
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ä»K230è®¾å¤‡è·å–çš„ä¿¡æ¯ï¼Œå¹¶å›ç­”ç›¸å…³é—®é¢˜ï¼š
        è®¾å¤‡è¿”å›å†…å®¹ï¼š{k230_text}
        
        è¦æ±‚ï¼š
        1. è¯†åˆ«å…¶ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆåŒ…æ‹¬ä¸­æ–‡é—®é¢˜ã€é”™è¯¯å…³é”®è¯ï¼‰ï¼›
        2. ç”¨è‡ªç„¶è¯­è¨€å›ç­”å…¶ä¸­çš„é—®é¢˜ï¼Œè§£é‡Šé”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰ï¼›
        3. å›å¤è¯­è¨€ä¸ºä¸­æ–‡ï¼Œç®€æ´æ˜“æ‡‚ã€‚
        """.strip()

        # 4. ç¼–ç è¾“å…¥
        conversation = [{"role": "user", "content": prompt}]
        inputs_text = tokenizer.apply_chat_template(
            conversation, tokenize=False, add_generation_prompt=True
        ) if args.weight != "pretrain" else tokenizer.bos_token + prompt
        
        inputs = tokenizer(
            inputs_text, return_tensors="pt", truncation=True, padding=True
        ).to(args.device)

        # 5. LLMç”Ÿæˆå›å¤
        print("\nğŸ¤–ï¸ LLMæ­£åœ¨ç”Ÿæˆå›å¤...")
        generated_ids = model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=args.max_new_tokens,
            do_sample=True,
            streamer=streamer,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            temperature=args.temperature,
            top_p=args.top_p,
            repetition_penalty=1.05,  # ä¸­æ–‡å»é‡
        )

        # 6. è§£ç å¹¶è¾“å‡ºå›å¤
        response = tokenizer.decode(
            generated_ids[0][len(inputs["input_ids"][0]):],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        # print(f"\nâœ… æœ€ç»ˆå›å¤:\n{response}")


if __name__ == "__main__":
    main()
    